{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing Efficiency by Generating more Heat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision Optimisations\n",
    "\n",
    "Unlike the less important transfomers from physics and the even less important transformers from the movies, our favourite transformer is one that actually grows in efficiency and performance as we generate more heat! The goal of this notebook will be to get the GPU cluster as hot as possible and complete as many computations in as short a time as possible.\n",
    "\n",
    "The first element of this that we will take advantage of is to drive down the precision of our parameters. Deep learning has progressively begun to use less and less precision in their parameters as the size of models have grown. This is not only because of the improvements in training efficiency from being able to complete magnitudes more FLOPS of computation (up to 16x more TFLOPS if we decide to train in BFLOAT16), but also because the model just becomes a lot smaller during inference time.\n",
    "\n",
    "Let's begin by pasting our current code base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected device: mps\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "@dataclass # adds all the dunder stuff to the class\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # Context length\n",
    "    vocab_size: int = 50257 # Word dictionary size\n",
    "    n_layer: int = 12 # Number of layers (blocks)\n",
    "    n_head: int = 12 # Number of heads\n",
    "    n_embd: int = 768 # Embedding dimension\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "print(f\"Detected device: {device}\")\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        with open('data/sample_input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        self.tokens = self.tokens.to(device)\n",
    "        print(\"num tokens in dataset:\", len(self.tokens))\n",
    "        print(\"batches to 1 epoch:\", len(self.tokens) // (self.B * self.T))\n",
    "\n",
    "        self.current_index = 0\n",
    "\n",
    "    def get_next_batch(self):\n",
    "        buf = self.tokens[self.current_index : self.current_index + self.B * self.T + 1]\n",
    "        x = buf[:-1].view(self.B, self.T)\n",
    "        y = buf[1:].view(self.B, self.T)\n",
    "        self.current_index += self.B * self.T\n",
    "\n",
    "        # Reset when out of bounds\n",
    "        if self.current_index + (self.B * self.T + 1) > len(self.tokens):\n",
    "            self.current_index = 0\n",
    "        return x, y\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_index = 0\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        # Confirm dimensions will be evenly distributed among all the heads\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # Create the key, query and value projections in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # Following info will need to be stored since forward pass needs to \n",
    "        # separate the abomination above\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_head = config.n_head\n",
    "        # The masked filter but idk why OpenAI called it bias.\n",
    "        # Resised to fit the above abomination\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1,1, config.block_size, config.block_size))\n",
    "        # Linear projection out of the Attention block\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Batch, time and channel of data (batch size, sequence length & emb dim)\n",
    "        B, T, C = x.size()\n",
    "        # Calculate the qkv value combined in the shape (B,T,3 * C)\n",
    "        qkv = self.c_attn(x)\n",
    "        # Split n_embd size bits out for k, q, v along the channel dimension\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        # Reshape each tensor to have the heads in dim=2 and then steal the weights for\n",
    "        # those heads by taking the values from the embedding dimension.\n",
    "        # Transpose the sequence length and head size so that the affinity calculation\n",
    "        # is completed on the sequence length * head size face of the matrices\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "        \n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(q.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1) # (B, n_heads, T, T)\n",
    "        y = att @ v # (B, n_heads, T, T) @ (B, n_heads, T, head_size) = (B, n_heads, T, head_sze)\n",
    "        # Re-orient tensor to shape (B, T, n_heads, head_size), followed by\n",
    "        # Concatenation via the view method\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        # Contiguous method ensures that the entire data is stored in a nice way\n",
    "        # such that no memory errors can occur when we do the concatenating.\n",
    "        # This is more important in our gpt-2 size model because our memory usage\n",
    "        # is high enough that our OS may split the memory to different places\n",
    "        \n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        # Expanding the dimensions of the data to let some \n",
    "        # computation occur\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        # gelu activation uses a tanh to approximate the real function\n",
    "        # in the GPT2 due to an error in PyTorch but we need the exact\n",
    "        # same stuff to import properly so we gotta suffer same way\n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "        # Projecting the data back into normal dimensions\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        \n",
    "        # Adding a custom flag to the model to flag our scaling\n",
    "        self.c_proj.CUSTOM_SCALING_INIT = 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Residual skip connection for the attention block with pre-norm\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        # Residual skip connection for the MLP block with pre-norm\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            # dictionary of token embedding weights (weight of token embeddings)\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            # dictionary of positional embedding weights (weight of positional embeddings)\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            # Attention blocks\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            # Final Layer norm (since pre-norm doesn't touch final block output)\n",
    "            ln_f = nn.LayerNorm(config.n_embd)\n",
    "        ))\n",
    "        # Linear projection out of the Attention block\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Weight sharing between wte and lm_head \n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        \n",
    "        # Apply weight initialisation\n",
    "        self.apply(self._init_weights) \n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        # Initialise the weights of the model using the same method as in GPT2\n",
    "        std = 0.02\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if hasattr(module, 'CUSTOM_SCALING_INIT'):\n",
    "                std = (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "    \n",
    "    def forward(self, text, targets=None):\n",
    "        # Raw data input of b context length sized sequences of raw text\n",
    "        B, T = text.size()\n",
    "        assert T <= self.config.block_size, \"Sequence length too high\"\n",
    "        # Create numbers from 0 to T\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=text.device)\n",
    "        # Fetch positional and token embeddings for all the values in text\n",
    "        pos_emb = self.transformer.wpe(pos) # of size (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(text) # of size (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # of size (B, T, vocab_size)\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tokens in dataset: 338025\n",
      "batches to 1 epoch: 41\n",
      "Step 1: Loss=11.0233 Time 18916.78ms Tokens/sec=433.05\n",
      "Step 2: Loss=9.5772 Time 19765.78ms Tokens/sec=414.45\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Enable humongous batch size on GPU\n",
    "# training_loader = DataLoader(B=16, T=1024)\n",
    "\n",
    "# Enable small batch size on CPU\n",
    "training_loader = DataLoader(B=8, T=1024)\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device);\n",
    "\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=25, gamma=0.1)\n",
    "steps = 2\n",
    "for i in range(steps):\n",
    "    t0 = time.time()\n",
    "    x, y = training_loader.get_next_batch()\n",
    "    optimiser.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    t1 = time.time()\n",
    "    # measure time taken for each step\n",
    "    dt = (t1 - t0) * 1000\n",
    "    # measure throughput of tokens/sec at each step\n",
    "    tokens_per_sec = (training_loader.B * training_loader.T) / (t1 - t0)\n",
    "    # scheduler.step()\n",
    "    print(f\"Step {i+1}: Loss={loss.item():.4f} Time={dt:.2f}ms tok/sec={tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by ensuring all the operaitons are done in a lower precision 32 bit mode. In normal 32 bit precision, the model trains with full 32 bit numbers, using the entire 24 mantissa bits to store the full precision. Using the \"high\" precision mode, we lower the usage of the mantisssa to only occupy 10 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this supposed to speed up the training's throughput by many multiples depending on the GPU you own and how many tensor cores are present inside. In our case on a Macbook, this is very negligible and almost nonexitstent. I will test later on hired GPUs but that's what we're seeing right now.\n",
    "\n",
    "Despite the above configuration downcasting all the float32's to TF32, the model still contains a bunch of 32 bit parameters flying around all over the place. The movement of all the data from place to place is still being done with the full float32 precision. This is why we were gaining some throughput (net amount of computations being done as seen in tok/s), our overall time per step was still high due to bandwidth limitations. Now let's tackle that by reducing the actual size of each parameter in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The details of how the TF and FP numbers are represented is very detailed and stuff, but to summarise, the FP numbers use all available bits to represent numbers while the TF numbers use slightly less to do their representation. This means that the TF numbers are more dense in a way. Have a look at the image below to see the difference.\n",
    "\n",
    "<image src=\"./notebooks/images/fp vs tf.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note is that the FP16 is on the standard set by the people who invented the idea of a float. The issue with FP16 is that it cuts the range of numbers possible on the float and that caused a lot of issues during training since casting between different precisions was super pain. BF16 was introduced in the Ampere series of GPUs and it's a 16 bit float that has a range of numbers that is similar to FP32 but cuts down on the number of precision bits in the mantissa.\n",
    "\n",
    "To make this happen in our model, we will use the autocast context manager in PyTorch. This function will automatically cast the parameters and inputs to the appropriate precision based on the operation that is being performed. This is a very powerful feature and it will help us reduce the size of each parameter in the model.\n",
    "\n",
    "Pytorch themselves reccomend for the autocast to only be applied to the forward pass and loss calculations in their documentation. Let's add that in to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "steps = 2\n",
    "for i in range(steps):\n",
    "    t0 = time.time()\n",
    "    x, y = training_loader.get_next_batch()\n",
    "    optimiser.zero_grad()\n",
    "    # Autocast forward pass parameters to BF16\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16): \n",
    "        logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    t1 = time.time()\n",
    "    # measure time taken for each step\n",
    "    dt = (t1 - t0) * 1000\n",
    "    # measure throughput of tokens/sec at each step\n",
    "    tokens_per_sec = (training_loader.B * training_loader.T) / (t1 - t0)\n",
    "    print(f\"Step {i+1}: Loss={loss.item():.4f} Time={dt:.2f}ms tok/sec={tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This autocast actually doesn't apply to every component of the forawrd pass, such as the wte and wpe. Pytoech decides which things require the conversions based what computational and performance effect they have on the forward pass. It's not actually clear what exactly pytorch decides not to autocast but we can trust them that they will pick what is most optimal (I think)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling for speed\n",
    "\n",
    "There's a torch compilation feature that compiles the neural network models, kind of like gcc does for C code. It applies a bunch of optimisations across the model and caches a bunch of stuff to make the model run faster. I have no idea what it does exactly but it says that it reduces the Python overhead and the GPU reads/ writes to speed up the model. \n",
    "\n",
    "For how it reduces python overhead, since we already algorithmically define what functions we will use from PyTorch and python and how they will be used, the compiler can basically hard code those elements into the model. By only taking the most important parts of libraries (things we use) and removing the majority of the python interpreter's role, the model is able to run faster (kind of like compiling multiple files of C code into one executable).\n",
    "\n",
    "As for how it reduces the number of read/ write operations, recall that GPUs have high bandwidth memory (HBM) in the same way that CPU's have RAM. Currently the GPU basically figures out kernels for what operations need to be completed and then sends those kernels to the GPU cores from the HBM when they need to be calculated. This is done for each step of the computation so something like input + 0.4 * torch.pow(input, 3.0) will make like 4 round trips to complete.\n",
    "\n",
    "The compile operation basically looks to find every operation compeleted by the model that looks like this which it can complete more efficently by keeping that value in the GPU cores without needing to repeatedly send it to the HBM. It basically finds what parts of the operation can be completed during the duration that the memory is in the GPU cores. This is called **kernel fusion**.\n",
    "\n",
    "This thing actually doesn't even work on MPS so we can't do it on the mac. Currently fighting between coding this entire thing on the cloud and just not running the code at all until I have all the code written locally. What a pain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device);\n",
    "# Compile the model\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now within a GPU, there is a little bit of extremely fast memory called **SRAM**. This memory is magnitudes faster than the HBM but as always, this speed demon is extremely small so it isn't used alot or most efficiently until a torch compile is completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flash Attention Algorithm\n",
    "A big breakthrough research paper invented the flash attention algorithm which rewrites the old attention operation softmax((q @ k^T)/sqrt(n_embd)) @ v with kernel fusion into 1 kernel.\n",
    "\n",
    "<image src=\"./notebooks/images/flash attention.png\" width=200>\n",
    "\n",
    "We make this change in code by simply replacing our manual attention calculation with the flash attention implementation inside PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        # Confirm dimensions will be evenly distributed among all the heads\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # Create the key, query and value projections in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # Following info will need to be stored since forward pass needs to \n",
    "        # separate the abomination above\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_head = config.n_head\n",
    "        # The masked filter but idk why OpenAI called it bias.\n",
    "        # Resised to fit the above abomination\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1,1, config.block_size, config.block_size))\n",
    "        # Linear projection out of the Attention block\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Batch, time and channel of data (batch size, sequence length & emb dim)\n",
    "        B, T, C = x.size()\n",
    "        # Calculate the qkv value combined in the shape (B,T,3 * C)\n",
    "        qkv = self.c_attn(x)\n",
    "        # Split n_embd size bits out for k, q, v along the channel dimension\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        # Reshape each tensor to have the heads in dim=2 and then steal the weights for\n",
    "        # those heads by taking the values from the embedding dimension.\n",
    "        # Transpose the sequence length and head size so that the affinity calculation\n",
    "        # is completed on the sequence length * head size face of the matrices\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "\n",
    "        # Efficient attention calculation using Flash Attention\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        # Re-orient tensor to shape (B, T, n_heads, head_size), followed by\n",
    "        # Concatenation via the view method\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        # Contiguous method ensures that the entire data is stored in a nice way\n",
    "        # such that no memory errors can occur when we do the concatenating.\n",
    "        # This is more important in our gpt-2 size model because our memory usage\n",
    "        # is high enough that our OS may split the memory to different places\n",
    "        \n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funniest Optimisation\n",
    "So you know how computers love the number 2 right? Well that's true for every calculation that happens inside the GPU too so using lots of numbers that are related to powers of 2 speeds things up a bit too!\n",
    "\n",
    "We now literally need to pass over some of those weird numbers in our codebase and change them to nice powers of 2.\n",
    "\n",
    "The first important number that looks weird is the vocab size of 50257, which is an odd number and can't be decomposed into nice powers of 2. The easiest way to fix numbers like these is to look to increase the number to the cleanest power of 2 number. In this case, we like the number 50304, which is divisible by 2, 4, 8, 16, 32 and 64! Those numbers sound much more computer like~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(device);\n",
    "# Compile the model\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will actually increase the number of flops of calculation that is completed but surpisingly it gives around a 5% increase in the speed! This is because the GPU usually breaks down problems into kernels that align to sizes of 2's, and then come back later after calculating those clean chunks to take the final leftover numbers and create a final few kernels to complete those operations. Hence those ugly numbers end up costing unnecessary time just to balance things out.\n",
    "\n",
    "The change basically introduces an extra few token embedding slots that aren't used. This means we waste a tiny miniscule amount of space for a big jump in the speed and efficiency of the model.\n",
    "\n",
    "Additionally, the higher vocab size means that the lm head at the end of the model has more parameters too and predicts a few more dimensions of possible tokens, albeit they most likely won't really be used for anything. The model will basically waste a bit of computation during training to learn that those last few tokens need to always have a probability of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this point, these optimisations alone have sped up the training by about 11x.\n",
    "\n",
    "Let's move on to algorithmic and hyperparameter opitimisations that will help us speed things up and increase the performance of the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Optimisations\n",
    "\n",
    "Let's begin by setting the parameters of the Adam optimiser to the same one that was used for GPT3. The default betas for the momentum and adaptive scaling is 0.9 and 0.999 by default, but we will set them to 0.9 and 0.95. The deafult epsilon at the bottom of the step calculation is 1*10^-8 and that is also what is used in the GPT3 paper but we will explicitly mention it anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to perform this thing called global norm clipping which basically adds a max limit to the norm of the gradients for the model. It calculates the norm by taking the squared sum of the gradients of every parameter in the model and then square roots that sum. It then makes sure that the final result is no larger than 1.\n",
    "\n",
    "The purpose of this is to avoid extremely edge case batches where the batch produces a super high loss which produces a really high gradient and then cause big changes to the models parameters which produces an overall \"shock\" to the models optimisation. In order to avoid this possibility, the gradient is clipped by the norm to a maximum of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 4\n",
    "\n",
    "for step in range(steps):\n",
    "    t0 = time.time()\n",
    "    x, y = training_loader.get_next_batch()\n",
    "    optimiser.zero_grad()\n",
    "    # Autocast forward pass parameters to BF16\n",
    "    # with torch.autocast(device_type=device, dtype=torch.bfloat16): \n",
    "    # MPS only supports FP16 not BF16 :(\n",
    "    with torch.autocast(device_type=device, dtype=torch.float16): \n",
    "        logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    # Clip the gradient norms to a maximum of 1\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimiser.step()\n",
    "    t1 = time.time()\n",
    "    # measure time taken for each step\n",
    "    dt = (t1 - t0) * 1000\n",
    "    # measure throughput of tokens/sec at each step\n",
    "    tokens_per_sec = (training_loader.B * training_loader.T) / (t1 - t0)\n",
    "    print(f\"Step {step+1}: Loss={loss.item():.4f} Time={dt:.2f}ms tok/sec={tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's add in a learning rate scheduler. In GPT3 they used a cosine decay learning schedule with warmup. This can be seen in the image below.\n",
    "\n",
    "<image src=\"./notebooks/images/cosine decay lr.png\" width=500>\n",
    "\n",
    "Let's implement it. In the GPT3 paper they say that they build the model with an initial learning rate around 6e-4 and then cosine decay down to 10% of that and then continue at that learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 10\n",
    "max_steps = 50\n",
    "def get_lr(step):\n",
    "    # Linear warmup for warmup steps\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * (step + 1) / warmup_steps\n",
    "    # Min learning rate after max steps\n",
    "    if step > max_steps:\n",
    "        return min_lr\n",
    "    # Cosine decay for the rest of the steps\n",
    "    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    # Coeff starts at 1 and goes to 0\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + (max_lr - min_lr) * coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(steps):\n",
    "    t0 = time.time()\n",
    "    x, y = training_loader.get_next_batch()\n",
    "    optimiser.zero_grad()\n",
    "    # Autocast forward pass parameters to BF16\n",
    "    # with torch.autocast(device_type=device, dtype=torch.bfloat16): \n",
    "    # MPS only supports FP16 not BF16 :(\n",
    "    with torch.autocast(device_type=device, dtype=torch.float16): \n",
    "        logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    # Clip the gradient norms to a maximum of 1\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    # Determine and set the learning rate for the current step\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimiser.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimiser.step()\n",
    "    t1 = time.time()\n",
    "    # measure time taken for each step\n",
    "    dt = (t1 - t0) * 1000\n",
    "    # measure throughput of tokens/sec at each step\n",
    "    tokens_per_sec = (training_loader.B * training_loader.T) / (t1 - t0)\n",
    "    print(f\"Step {step+1}: Loss={loss.item():.4f} Time={dt:.2f}ms tok/sec={tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the model actually utilises a bit of weight decay in some of the layers for regularisation. Recall that applying weight decay means that weights are penalised for getting too large, which forces the model to better utilise the entire network during optimisation. This is done for most of the parameters except for some layers like layer norms and biases are left alone. This simplifies down to only applying weight decay to parameter sets that have 2 or more dimensions.\n",
    "\n",
    "Weight decay is not applied to things like layer norms and biases because it affects their performance negatively since we don't particularly need those elements of the model to be regularised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            # dictionary of token embedding weights (weight of token embeddings)\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            # dictionary of positional embedding weights (weight of positional embeddings)\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            # Attention blocks\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            # Final Layer norm (since pre-norm doesn't touch final block output)\n",
    "            ln_f = nn.LayerNorm(config.n_embd)\n",
    "        ))\n",
    "        # Linear projection out of the Attention block\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Weight sharing between wte and lm_head \n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        \n",
    "        # Apply weight initialisation\n",
    "        self.apply(self._init_weights) \n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        # Initialise the weights of the model using the same method as in GPT2\n",
    "        std = 0.02\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if hasattr(module, 'CUSTOM_SCALING_INIT'):\n",
    "                std = (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            \n",
    "    def set_optimiser(self, weight_decay, learning_rate, device):\n",
    "        # Fetch all parameters that require grad\n",
    "        params = {pn: p for pn, p in self.named_parameters()}\n",
    "        params = {pn: p for pn, p in params.items() if p.requires_grad}\n",
    "        \n",
    "        # Create groups for parameters that require weight decay\n",
    "        decay_params = [p for n, p in params.items() if p.dim() >= 2]\n",
    "        no_decay_params = [p for n, p in params.items() if p.dim() < 2]\n",
    "        grouped_params = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': no_decay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        \n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_no_decay_params = sum(p.numel() for p in no_decay_params)\n",
    "        print(f\"Number of decay params: {num_decay_params}\")\n",
    "        print(f\"Number of no decay params: {num_no_decay_params}\")\n",
    "        \n",
    "        # Create AdamW optimiser with fused AdamW if available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and 'cuda' in device\n",
    "        print(f\"Using fused AdamW: {use_fused}\")\n",
    "        optimiser = torch.optim.AdamW(grouped_params,\n",
    "                                        lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        return optimiser\n",
    "    \n",
    "    def forward(self, text, targets=None):\n",
    "        # Raw data input of b context length sized sequences of raw text\n",
    "        B, T = text.size()\n",
    "        assert T <= self.config.block_size, \"Sequence length too high\"\n",
    "        # Create numbers from 0 to T\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=text.device)\n",
    "        # Fetch positional and token embeddings for all the values in text\n",
    "        pos_emb = self.transformer.wpe(pos) # of size (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(text) # of size (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # of size (B, T, vocab_size)\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the new set_optimiser function that we used the fused AdamW if it is available. What this does is that it applies a big optimisation on the backprop for the parameters by bunching the params in to a larger kernel rather than iterating over each param group and creating a kernel for each of those. This also grants us a 3% boost in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = model.set_optimiser(weight_decay=0.1, learning_rate=6e-4, device=device)\n",
    "\n",
    "for step in range(steps):\n",
    "    t0 = time.time()\n",
    "    x, y = training_loader.get_next_batch()\n",
    "    optimiser.zero_grad()\n",
    "    # Autocast forward pass parameters to BF16\n",
    "    # with torch.autocast(device_type=device, dtype=torch.bfloat16): \n",
    "    # MPS only supports FP16 not BF16 :(\n",
    "    with torch.autocast(device_type=device, dtype=torch.float16): \n",
    "        logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    # Clip the gradient norms to a maximum of 1\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    # Determine and set the learning rate for the current step\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimiser.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimiser.step()\n",
    "    t1 = time.time()\n",
    "    # measure time taken for each step\n",
    "    dt = (t1 - t0) * 1000\n",
    "    # measure throughput of tokens/sec at each step\n",
    "    tokens_per_sec = (training_loader.B * training_loader.T) / (t1 - t0)\n",
    "    print(f\"Step {step+1}: Loss={loss.item():.4f} Time={dt:.2f}ms tok/sec={tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Accumulation\n",
    "\n",
    "Now all the hyperparameters we set was selected under the assumption that we would have about 0.5 million tokens per batch. Now that would need us to have nearly 500 token sequences simultaneously in the same batch but even if we use 8xH100's, we wouldn't be able to  fit all that in the GPUs. Hence, we will use this thing called gradient accumulation, which allows us to simulate arbitrarily larger batch sizes by sequentially calculating enough mini batches to reach the target batch size before performing backprop on the accumulated gradient from the mini batches.\n",
    "\n",
    "Let's begin by rounding up that 500,000 token number to a cleaner number that is nicely divisible. Following that, we'll calculate how many mini-mini batches we will need to accumulate before we reach the required mini batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient accumulation steps: 32\n"
     ]
    }
   ],
   "source": [
    "total_batch_tokens = 524288\n",
    "B = 16\n",
    "T = 1024\n",
    "\n",
    "assert total_batch_tokens % (B*T) == 0, \"Total batch size must be divisible by batch size\"\n",
    "\n",
    "grad_accum_steps = total_batch_tokens // (B*T)\n",
    "print(f\"Gradient accumulation steps: {grad_accum_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's fix up our training loop to account for gradient accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    optimiser.zero_grad()\n",
    "    accumulated_loss = 0.0\n",
    "    # Accumulate gradient prior to optimiser step\n",
    "    for mini_step in range(grad_accum_steps):\n",
    "        x, y = training_loader.get_next_batch()\n",
    "        # Autocast forward pass parameters to BF16 (Enable when CUDA)\n",
    "        # with torch.autocast(device_type=device, dtype=torch.bfloat16): \n",
    "        # MPS only supports FP16 not BF16 :(\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16): \n",
    "            logits, loss = model(x, y)\n",
    "\n",
    "        # Ensure that loss normalisation is still present after accumulation\n",
    "        loss = loss / grad_accum_steps\n",
    "        accumulated_loss += loss.detach()\n",
    "        loss.backward()\n",
    "\n",
    "    # Clip the gradient norms to a maximum of 1\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # Determine and set the learning rate for the current step\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimiser.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimiser.step()\n",
    "    t1 = time.time()\n",
    "\n",
    "    # measure time taken for each step\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_processed = training_loader.B * training_loader.T * grad_accum_steps\n",
    "    # measure throughput of tokens/sec at each step\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "    print(f\"Step {step+1}: Loss={accumulated_loss.item():.4f} Time={dt:.2f}ms tok/sec={tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the loss is divided by the number of accumulation steps we complete. This is because we usually divide the loss for each step by the number of examples in the batch so that the loss is like batch_loss = 1/batch_size * sum(loss(example_i)). This means that when we accumulate gradients, this sum gets lost, so we have to manually add it back in.\n",
    "\n",
    "Additionally we added a few variables that will help us track the loss, tok/s and stuff properly over each mini batch of 0.5M tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU Parallelisation\n",
    "Time for the big weapons of optimisation. We are gonnna begin with GPU parrallelisation. When we train, we will hire out 8 H100's so we will be able to utilise the entirety of those GPUs to train our model. We will basically make each of the 8 GPUs take a batch of data and compute a batch loss on them. This loss will then be averaged over the 8 GPUs to find the final gradient with which the optimiser step is taken. Let's add the code that will setup the Distributed Data Parallel (DDP) environment if it is available.\n",
    "\n",
    "In a DDP environment, the rank is the identifier of which GPU in the current environment is being used, while the World Size describes how many GPU's are availaable, and hence the upper limit of what the rank of a DDP environment can be. If we had multiple nodes, we would have also had a local_rank to descripe which node is being referred to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.distributed import init_process_group\n",
    "\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # Check if multiple GPUs are available\n",
    "if ddp:\n",
    "    assert torch.cuda.is_available(), \"DDP requires CUDA\"\n",
    "    init_process_group(backend='nccl')\n",
    "    ddp_rank = int(os.environ('RANK'))\n",
    "    ddp_world_size = int(os.environ('WORLD_SIZE'))\n",
    "    ddp_local_rank = int(os.environ('LOCAL_RANK'))\n",
    "    device = f\"cuda:{ddp_local_rank}\"\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0\n",
    "    print(f\"Running with DDP on {ddp_world_size} GPUs\")\n",
    "else:\n",
    "    ddp_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    ddp_local_rank = 0\n",
    "    master_process = True\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    print(f\"Running on device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each GPU instance, at each rank, the GPU will run the exact same piece of code, have it's own identical Python interpretor, running its own training loop.\n",
    "\n",
    "Since each GPU will run one mini- mini batch, we will need to readjust our gradient accumulation steps too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, context_length = 8, 1024\n",
    "\n",
    "# Set up gradient accumulation\n",
    "total_batch_tokens = 524288\n",
    "\n",
    "assert total_batch_tokens % (batch_size * context_length * ddp_world_size) == 0, \"Total batch size must be divisible by mini batch size\"\n",
    "\n",
    "grad_accum_steps = total_batch_tokens // (batch_size*context_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now with 8 GPU's this is about 4 accumulating mini batches to reach the full batch size.\n",
    "\n",
    "Now since there will be a parrallel process running on each GPU, we need to fix up our data loader to make sure that it does not fetch the exact same data batch for each of the process, since we have manual seeding right now.\n",
    "\n",
    "We will basically just offset the data loader by 1 based on the process number so that each process is like 1 load ahead of it's previous process. We move in chunks but we still traverse the data the same way since our seeding between each process is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, B, T, process_rank, num_processes):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        with open('data/sample_input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        self.tokens = self.tokens.to(device)\n",
    "        print(\"num tokens in dataset:\", len(self.tokens))\n",
    "        print(\"batches to 1 epoch:\", len(self.tokens) // (self.B * self.T))\n",
    "\n",
    "        # Calculate the starting index based on offset using process rank\n",
    "        self.current_index = self.B * self.T * self.process_rank\n",
    "\n",
    "    def get_next_batch(self):\n",
    "        buf = self.tokens[self.current_index : self.current_index + self.B * self.T + 1]\n",
    "        x = buf[:-1].view(self.B, self.T)\n",
    "        y = buf[1:].view(self.B, self.T)\n",
    "        \n",
    "        # Update the current index for the next batch based on process\n",
    "        self.current_index += self.B * self.T * self.process_rank\n",
    "\n",
    "        # Reset when out of bounds\n",
    "        if self.current_index + (self.B * self.T * self.num_processes + 1) > len(self.tokens):\n",
    "            self.current_index = self.B * self.T * self.process_rank\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to wrap our model in the ddp container. This container doesn't do anything different in the forward pass but in the backward pass, it calculates an average gradient for all the parameters across all the GPUs and then deposits that average to all the GPU's.\n",
    "\n",
    "Infact, the DDP manager actually also dispatches averages for parameters from the other GPUs/ ranks while they are still computing the backward pass. So if one layer has been completed across all GPUs, it may start to dispatch the average gradients for those layers without waiting for every rank to complete their backward pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(device);\n",
    "# Compile the model (on GPU) to remove Python overhead\n",
    "# model = torch.compile(model)\n",
    "\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "raw_model = model.module if ddp else model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we wrap the model with a DDP manager, we will need to make sure that the optimiser makes changes to the raw model inside the DDP wrapped model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = raw_model.set_optimiser(weight_decay=0.1, learning_rate=6e-4, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the dispatching averages, with the current setup, it will dispatch and synchronise across the ranks on every loss.backward() but we don't need syncing that frequently. Doing the sync once on the very last mini batch is enough. Because of this, we will enable a special flag on the loss.backward() that synchronises the gradients only during the final mini batch and disabled at all other times.\n",
    "\n",
    "Note that we also need to fix up our accumulated loss tracker and the tokens per second to account for all the other GPU's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    optimiser.zero_grad()\n",
    "    accumulated_loss = 0.0\n",
    "    # Accumulate gradient prior to optimiser step\n",
    "    for mini_step in range(grad_accum_steps):\n",
    "        x, y = training_loader.get_next_batch()\n",
    "        # Autocast forward pass parameters to BF16 (Enable when CUDA)\n",
    "        # with torch.autocast(device_type=device, dtype=torch.bfloat16): \n",
    "        # MPS only supports FP16 not BF16 :(\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16): \n",
    "            logits, loss = model(x, y)\n",
    "\n",
    "        # Ensure that loss normalisation is still present after accumulation\n",
    "        loss = loss / grad_accum_steps\n",
    "        accumulated_loss += loss.detach()\n",
    "        # Only sync gradients on the last mini batch\n",
    "        if ddp:\n",
    "            model.require_backward_grad_sync = (mini_step == grad_accum_steps - 1)\n",
    "        loss.backward()\n",
    "    # Ensure that the displayed loss is averaged across all the GPUs\n",
    "    if ddp:\n",
    "        torch.distributed.all_reduce(accumulated_loss, op=torch.distributed.ReduceOp.AVG) \n",
    "\n",
    "    # Clip the gradient norms to a maximum of 1\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # Determine and set the learning rate for the current step\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimiser.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimiser.step()\n",
    "    t1 = time.time()\n",
    "\n",
    "    # measure time taken for each step\n",
    "    dt = (t1 - t0) * 1000\n",
    "    # Calculate tokens processed across all GPUs\n",
    "    tokens_processed = training_loader.B * training_loader.T * grad_accum_steps * ddp_world_size\n",
    "    # measure throughput of tokens/sec at each step\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "    print(f\"Step {step+1}: Loss={accumulated_loss.item():.4f} Time={dt:.2f}ms tok/sec={tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with that, we acutally have the optimised model ready for hardcore training.\n",
    "\n",
    "I have already copied a fineweb dataset fetcher file. In order to train our model, we just need to run the optimised-gpt2-train.py file in a GPU environment and we're good to go.\n",
    "\n",
    "We need to make a couple of changes to our data loading process to now be able to work with the shards of data we have. We also additionally add a reset function so that we can run validation on the model during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_tokens(filename):\n",
    "    np_tensor = np.load(filename)\n",
    "    pt_tensor = torch.tensor(np_tensor, dtype=torch.long)\n",
    "    return pt_tensor\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, B, T, process_rank, num_processes, split):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        assert split in [\"train\", \"val\"]\n",
    "        \n",
    "        # Get the shard filenames\n",
    "        data_root = \"edu_fineweb10B\"\n",
    "        shards = os.listdir(data_root)\n",
    "        shards = [shard for shard in shards if split in shard]\n",
    "        shards = sorted(shards)\n",
    "        shards = [os.path.join(data_root, shard) for shard in shards]\n",
    "        self.shards = shards\n",
    "        \n",
    "        assert len(self.shards) > 0, \"No shards found\"\n",
    "        if master_process:\n",
    "            print(f\"Found {len(self.shards)} shards for {split} split\")\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def get_next_batch(self):\n",
    "        buf = self.tokens[self.current_index : self.current_index + self.B * self.T + 1]\n",
    "        x = buf[:-1].view(self.B, self.T)\n",
    "        y = buf[1:].view(self.B, self.T)\n",
    "        \n",
    "        # Update the current index for the next batch based on process\n",
    "        self.current_index += self.B * self.T * self.num_processes\n",
    "\n",
    "        # Reset when out of bounds\n",
    "        if self.current_index + (self.B * self.T * self.num_processes + 1) > len(self.tokens):\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "            self.current_index = self.B * self.T * self.process_rank\n",
    "        return x, y\n",
    "    \n",
    "    def reset(self):\n",
    "        # State init at shard 0\n",
    "        self.current_shard = 0\n",
    "        # Calculate the starting index based on offset using process rank\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_index = self.B * self.T * self.process_rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(batch_size, context_length, ddp_rank, ddp_world_size, split=\"train\")\n",
    "validation_loader = DataLoader(batch_size, context_length, ddp_rank, ddp_world_size, split=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then let's fix up our training loop to do validation while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Validate every 100 steps\n",
    "    if step % 100 == 0:\n",
    "        model.eval()\n",
    "        validation_loader.reset()\n",
    "        with torch.no_grad():\n",
    "            accumulated_val_loss = 0.0\n",
    "            val_loss_steps = 20\n",
    "            for _ in range(val_loss_steps):\n",
    "                x, y = validation_loader.get_next_batch()\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16): \n",
    "                    logits, loss = model(x, y)\n",
    "                loss = loss / val_loss_steps\n",
    "                accumulated_val_loss += loss.detach()\n",
    "        if ddp:\n",
    "            torch.distributed.all_reduce(accumulated_val_loss, op=torch.distributed.ReduceOp.AVG)\n",
    "        if master_process:\n",
    "            print(f\"Validation loss: {accumulated_val_loss.item():.4f}\")\n",
    "            \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    optimiser.zero_grad()\n",
    "    accumulated_loss = 0.0\n",
    "    # Accumulate gradient prior to optimiser step\n",
    "    for mini_step in range(grad_accum_steps):\n",
    "        x, y = training_loader.get_next_batch()\n",
    "        # Autocast forward pass parameters to BF16 (Enable when CUDA) !!!\n",
    "        # with torch.autocast(device_type=device, dtype=torch.bfloat16): \n",
    "        # MPS only supports FP16 not BF16 :(\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16): \n",
    "            logits, loss = model(x, y)\n",
    "\n",
    "        # Ensure that loss normalisation is still present after accumulation\n",
    "        loss = loss / grad_accum_steps\n",
    "        accumulated_loss += loss.detach()\n",
    "        # Only sync gradients on the last mini batch\n",
    "        if ddp:\n",
    "            model.require_backward_grad_sync = (mini_step == grad_accum_steps - 1)\n",
    "        loss.backward()\n",
    "    # Ensure that the displayed loss is averaged across all the GPUs\n",
    "    if ddp:\n",
    "        torch.distributed.all_reduce(accumulated_loss, op=torch.distributed.ReduceOp.AVG) \n",
    "\n",
    "    # Clip the gradient norms to a maximum of 1\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # Determine and set the learning rate for the current step\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimiser.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimiser.step()\n",
    "    t1 = time.time()\n",
    "\n",
    "    # measure time taken for each step\n",
    "    dt = (t1 - t0) * 1000\n",
    "    # Calculate tokens processed across all GPUs\n",
    "    tokens_processed = training_loader.B * training_loader.T * grad_accum_steps * ddp_world_size\n",
    "    # measure throughput of tokens/sec at each step\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "    print(f\"Step {step+1}: loss={accumulated_loss.item():.4f} norm={norm:.4f} t={dt:.2f}ms tok/sec={tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll set up the model configs to match the same ratio as in GPT3. We're shooting for same ratio because our training is on way smaller dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up learning rate scheduler\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 715 # Same ratio of warmup as GPT3\n",
    "max_steps = 19073 # Exactly 1 epoch over the 10B token dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're at it, let's also make it so our model samples a little bit every now and then too so that we can visually see how the generation improves as the training proceeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Validate every 100 steps\n",
    "    if step % 100 == 0:\n",
    "        model.eval()\n",
    "        validation_loader.reset()\n",
    "        with torch.no_grad():\n",
    "            accumulated_val_loss = 0.0\n",
    "            val_loss_steps = 20\n",
    "            for _ in range(val_loss_steps):\n",
    "                x, y = validation_loader.get_next_batch()\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16): \n",
    "                    logits, loss = model(x, y)\n",
    "                loss = loss / val_loss_steps\n",
    "                accumulated_val_loss += loss.detach()\n",
    "        if ddp:\n",
    "            torch.distributed.all_reduce(accumulated_val_loss, op=torch.distributed.ReduceOp.AVG)\n",
    "        if master_process:\n",
    "            print(f\"Validation loss: {accumulated_val_loss.item():.4f}\")\n",
    "    \n",
    "    # Generate every 250 steps\n",
    "    if ((step > 0 and step % 250 == 0) or step == max_steps - 1):\n",
    "        model.eval()\n",
    "        num_return_sequences = 4\n",
    "        max_length = 32\n",
    "        tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "        xgen = tokens.to(device)\n",
    "        sample_rng = torch.Generator(device=device)\n",
    "        sample_rng.manual_seed(42 + ddp_rank)\n",
    "        while xgen.size(1) < max_length:\n",
    "            # forward the model to get the logits\n",
    "            with torch.no_grad():\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                    logits, loss = model(xgen) # (B, T, vocab_size)\n",
    "                # take the logits at the last position\n",
    "                logits = logits[:, -1, :] # (B, vocab_size)\n",
    "                # get the probabilities\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                # do top-k sampling of 50 (huggingface pipeline default)\n",
    "                # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "                topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "                # select a token from the top-k probabilities\n",
    "                # note: multinomial does not demand the input to sum to 1\n",
    "                ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "                # gather the corresponding indices\n",
    "                xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "                # append to the sequence\n",
    "                xgen = torch.cat((xgen, xcol), dim=1)\n",
    "        # print the generated text\n",
    "        for i in range(num_return_sequences):\n",
    "            tokens = xgen[i, :max_length].tolist()\n",
    "            decoded = enc.decode(tokens)\n",
    "            print(f\"rank {ddp_rank} sample {i}: {decoded}\")\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    optimiser.zero_grad()\n",
    "    accumulated_loss = 0.0\n",
    "    # Accumulate gradient prior to optimiser step\n",
    "    for mini_step in range(grad_accum_steps):\n",
    "        x, y = training_loader.get_next_batch()\n",
    "        # Autocast forward pass parameters to BF16 (Enable when CUDA) !!!\n",
    "        # with torch.autocast(device_type=device, dtype=torch.bfloat16): \n",
    "        # MPS only supports FP16 not BF16 :(\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16): \n",
    "            logits, loss = model(x, y)\n",
    "\n",
    "        # Ensure that loss normalisation is still present after accumulation\n",
    "        loss = loss / grad_accum_steps\n",
    "        accumulated_loss += loss.detach()\n",
    "        # Only sync gradients on the last mini batch\n",
    "        if ddp:\n",
    "            model.require_backward_grad_sync = (mini_step == grad_accum_steps - 1)\n",
    "        loss.backward()\n",
    "    # Ensure that the displayed loss is averaged across all the GPUs\n",
    "    if ddp:\n",
    "        torch.distributed.all_reduce(accumulated_loss, op=torch.distributed.ReduceOp.AVG) \n",
    "\n",
    "    # Clip the gradient norms to a maximum of 1\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # Determine and set the learning rate for the current step\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimiser.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimiser.step()\n",
    "    t1 = time.time()\n",
    "\n",
    "    # measure time taken for each step\n",
    "    dt = (t1 - t0) * 1000\n",
    "    # Calculate tokens processed across all GPUs\n",
    "    tokens_processed = training_loader.B * training_loader.T * grad_accum_steps * ddp_world_size\n",
    "    # measure throughput of tokens/sec at each step\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "    if master_process:\n",
    "        print(f\"Step {step+1}: loss={accumulated_loss.item():.4f} norm={norm:.4f} t={dt:.2f}ms tok/sec={tokens_per_sec:.2f}\")\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"{step} train {accumulated_loss.item():.6f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
