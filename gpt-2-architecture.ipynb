{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's GODDAMN make GPT2!\n",
    "We want to have the exact same model for this instance but we want to try achieve better performance than the original GPT-2 by curating cleaner training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in order to be able to straight up pull down the code from HF and still be able to use it in our model, we will need to make sure we stick very closely to the originals code too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass # adds all the dunder stuff to the class\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # Context length\n",
    "    vocab_size: int = 50257 # Word dictionary size\n",
    "    n_layer: int = 12 # Number of layers (blocks)\n",
    "    n_head: int = 12 # Number of heads\n",
    "    n_embd: int = 768 # Embedding dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while coding the self attention heads and the multi-headed attention separately is much cleaner, the following is much more computationally efficient. I'm sorry to anyone that has to read this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        # Confirm dimensions will be evenly distributed among all the heads\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # Create the key, query and value projections in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # Following info will need to be stored since forward pass needs to \n",
    "        # separate the abomination above\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_head = config.n_head\n",
    "        # The masked filter but idk why OpenAI called it bias.\n",
    "        # Resised to fit the above abomination\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1,1, config.block_size, config.block_size))\n",
    "        # Linear projection out of the Attention block\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Batch, time and channel of data (batch size, sequence length & emb dim)\n",
    "        B, T, C = x.size()\n",
    "        # Calculate the qkv value combined in the shape (B,T,3 * C)\n",
    "        qkv = self.c_attn(x)\n",
    "        # Split n_embd size bits out for k, q, v along the channel dimension\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        # Reshape each tensor to have the heads in dim=2 and then steal the weights for\n",
    "        # those heads by taking the values from the embedding dimension.\n",
    "        # Transpose the sequence length and head size so that the affinity calculation\n",
    "        # is completed on the sequence length * head size face of the matrices\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "        \n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(q.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1) # (B, n_heads, T, T)\n",
    "        y = att @ v # (B, n_heads, T, T) @ (B, n_heads, T, head_size) = (B, n_heads, T, head_sze)\n",
    "        # Re-orient tensor to shape (B, T, n_heads, head_size), followed by\n",
    "        # Concatenation via the view method\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        # Contiguous method ensures that the entire data is stored in a nice way\n",
    "        # such that no memory errors can occur when we do the concatenating.\n",
    "        # This is more important in our gpt-2 size model because our memory usage\n",
    "        # is high enough that our OS may split the memory to different places\n",
    "        \n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gist of the code above is that we are able to compute the qkv all as one since our application of the q, k and v are all done using the same parameter on each of the tokens individually. Note that since we compute multiple self attention heads at the same time to concatenate the results back to n_embd afterwards, we can actually just, project the input channel dimensions up to 3 * n_embd instead, which avoids all the middle steps.\n",
    "\n",
    "The core calculation to maintain is that of calculating the affinities, which is the actual attention component. Hence, we divide out the q, k and v into the target head dimensions while transposing the 1st and 2nd dimensions to ensure our calculations are performed correctly. This ends up producing q, k, v matrices which have b batches of data chunks, where each chunk has been processed into n_head sequences of tokens, where each token is represented with head_size dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        # Expanding the dimensions of the data to let some \n",
    "        # computation occur\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        # gelu activation uses a tanh to approximate the real function\n",
    "        # in the GPT2 due to an error in PyTorch but we need the exact\n",
    "        # same stuff to import properly so we gotta suffer same way\n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "        # Projecting the data back into normal dimensions\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Residual skip connection for the attention block with pre-norm\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        # Residual skip connection for the MLP block with pre-norm\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of stuff happening below but it's not horribly different to the regular old GPT architecture. The key difference is in the way we had to code it up to make sure that we could easily load the OpenAI model weights onto it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            # dictionary of token embedding weights (weight of token embeddings)\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            # dictionary of positional embedding weights (weight of positional embeddings)\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            # Attention blocks\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            # Final Layer norm (since pre-norm doesn't touch final block output)\n",
    "            ln_f = nn.LayerNorm(config.n_embd)\n",
    "        ))\n",
    "        # Linear projection out of the Attention block\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_hf(cls, model_type):\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        config_args = {\n",
    "            'gpt2': dict(n_layer=12, n_head=12, n_embd=768), # 124M params\n",
    "            'gpt2-medium': dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large': dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl': dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args[\"vocab_size\"] = 50257\n",
    "        config_args[\"block_size\"] = 1024\n",
    "        \n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        # Remove the non-parameter components\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith(\".attn.bias\")]\n",
    "        \n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        \n",
    "        # Copy all the parameter labels from our custom class in order to\n",
    "        # paste in \n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        # OpenAI used 1D convs on the data, which meant they had to make the T\n",
    "        # dimension the final one, which is why in our usage, we will need to undo\n",
    "        # this transpose.\n",
    "        # The most likely purpose of this was in order to create a projection that\n",
    "        # retained more of the local infomration\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        \n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"Bad keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def forward(self, text):\n",
    "        # Raw data input of b context length sized sequences of raw text\n",
    "        B, T = text.size()\n",
    "        assert T <= self.config.block_size, \"Sequence length too high\"\n",
    "        # Create numbers from 0 to T\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=text.device)\n",
    "        # Fetch positional and token embeddings for all the values in text\n",
    "        pos_emb = self.transformer.wpe(pos) # of size (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(text) # of size (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # of size (B, T, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you see this, it took me 2 hours of bug fixing before this finally printed\n"
     ]
    }
   ],
   "source": [
    "# model = GPT.from_hf('gpt2')\n",
    "model = GPT(GPTConfig()) # Enable this to use a randomly initialised model\n",
    "print(\"If you see this, it took me 2 hours of bug fixing before this finally printed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected device: mps\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"Detected device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22087, 502, 502, 502]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "max_length = 25\n",
    "num_return_sequences = 5\n",
    "\n",
    "# use the online tokeniser for gpt 2\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"Help me me me\")\n",
    "print(tokens)\n",
    "# create a tensor of the input tokens\n",
    "input_tokens = torch.tensor(tokens[:-1], dtype=torch.long) # of size (2)\n",
    "target_tokens = torch.tensor(tokens[1:], dtype=torch.long)\n",
    "# duplicate tokens across tensor to generate multiple predictions\n",
    "input_tokens = input_tokens.unsqueeze(0).repeat(num_return_sequences, 1) # of size (5, 2)\n",
    "target_tokens = target_tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "x = input_tokens.to(device)\n",
    "y = target_tokens.to(device)\n",
    "# print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1: Help me me rein voltsArm colored Hicks prince fet Diss Font RevelationstilractedFulletti hints 367CompApparentlyVIILLOWenegger Jeanne\n",
      "Generation 2: Help me me Aryato championed SovereignOverviewmanufact Moduleively invests docker changesarded participate Ant 1925 longtime grandparents Lect unlike Congratulationsjay }\n",
      "Generation 3: Help me meton typed Doll presenter erroneous oversising subsequently innumerable ComeorrowORT Meteor LEVEL cowardlyalt Caucus disbanded decentralized headlines Mum Kappa\n",
      "Generation 4: Help me meanimalAZ revival Ras� affordabilityatalie IPM messed Ice480 impede regimen compiled tornado Trout Absolute appoint French 96 Lep insanely\n",
      "Generation 5: Help me me hijacked hopeired findingOr� Sands SHARES Kw leakingJoyageddon Hod enforce sonic collector 243classified principled Lab SalemDetroit\n"
     ]
    }
   ],
   "source": [
    "while x.size(1) < max_length:\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        # Take the logits at the last position for the entire batch\n",
    "        # with the entire vocab_size\n",
    "        logits= logits[:, -1, :]\n",
    "        # Softmax over the vocab_size to prepare for sampling\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # Since we want to generate exactly as hugging face does\n",
    "        # below code is the same as HF.\n",
    "        # Isolate the top 50 most likely words to avoid even super improbable\n",
    "        # words from being a candidate. This avoids model random slip ups\n",
    "        top_k_probs, top_k_indices = torch.topk(probs, 50, dim=-1) # of size (5,50)\n",
    "        # Sample one random word from the top 50\n",
    "        chosen_index = torch.multinomial(top_k_probs, 1) # of size (B, 1)\n",
    "        # Find corresponding indices of chosen indexes\n",
    "        xcol = torch.gather(top_k_indices, -1, chosen_index)\n",
    "        # Put all the collected indexes together\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(f\"Generation {i+1}:\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this model above is using the pretrained weights from the HF model. We can enable the random initialisation version too but it's just gonna print gibberish till we've trained it obviously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the training loop\n",
    "\n",
    "So now that we know a bit about how to perform inference with the model, let's build the training loop. We will build out the data loader first and then move on to building out the rest of the functions that will make up the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        with open('data/sample_input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        self.tokens = self.tokens.to(device)\n",
    "        print(\"num tokens in dataset:\", len(self.tokens))\n",
    "        print(\"batches to 1 epoch:\", len(self.tokens) // (self.B * self.T))\n",
    "\n",
    "        self.current_index = 0\n",
    "\n",
    "    def get_next_batch(self):\n",
    "        buf = self.tokens[self.current_index : self.current_index + self.B * self.T + 1]\n",
    "        x = buf[:-1].view(self.B, self.T)\n",
    "        y = buf[1:].view(self.B, self.T)\n",
    "        self.current_index += self.B * self.T\n",
    "        # Reset when out of bounds\n",
    "        if self.current_index + (self.B * self.T + 1) > len(self.tokens):\n",
    "            self.current_index = 0\n",
    "        return x, y\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_index = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the process by which logits will be computed. For this we will first need to modify the forward pass to make sure it not only returns the logits but also the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            # dictionary of token embedding weights (weight of token embeddings)\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            # dictionary of positional embedding weights (weight of positional embeddings)\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            # Attention blocks\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            # Final Layer norm (since pre-norm doesn't touch final block output)\n",
    "            ln_f = nn.LayerNorm(config.n_embd)\n",
    "        ))\n",
    "        # Linear projection out of the Attention block\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, text, targets=None):\n",
    "        # Raw data input of b context length sized sequences of raw text\n",
    "        B, T = text.size()\n",
    "        assert T <= self.config.block_size, \"Sequence length too high\"\n",
    "        # Create numbers from 0 to T\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=text.device)\n",
    "        # Fetch positional and token embeddings for all the values in text\n",
    "        pos_emb = self.transformer.wpe(pos) # of size (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(text) # of size (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # of size (B, T, vocab_size)\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tokens in dataset: 338025\n",
      "batches to 1 epoch: 2640\n",
      "Initial loss:  tensor(11.0450, device='mps:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = GPT(GPTConfig())\n",
    "model.to(device);\n",
    "training_loader = DataLoader(B=4, T=32)\n",
    "x, y = training_loader.get_next_batch()\n",
    "logits, loss = model(x, y)\n",
    "\n",
    "print(\"Initial loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An extended perspective on initialisation which becomes more important as the model size increases is that the initialisation of the chance for any given token to be the next predicted token should be as close to equally distributed as possible. This is because we don't really want the model to be confidently wrong in prediction anything in the beginning. An approximately equal distribution of logits can be calculated by considering 1/vocab_size. This means we want to begin at maximum entropy. Hence the initial log loss should be as close to -log(1/vocab_size) as possible, which in this case is around -log(1/50257) = 10.8249. A loss like this tells us that the model has about random guess confidence.\n",
    "\n",
    "Now let's have a look at getting the optimiser ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loss 6.812870502471924\n",
      "Step 2: Loss 7.213290214538574\n",
      "Step 3: Loss 6.311725616455078\n",
      "Step 4: Loss 6.419493675231934\n",
      "Step 5: Loss 6.509957313537598\n",
      "Step 6: Loss 6.604783058166504\n",
      "Step 7: Loss 6.519889831542969\n",
      "Step 8: Loss 6.748194694519043\n",
      "Step 9: Loss 6.777368545532227\n",
      "Step 10: Loss 7.473328590393066\n",
      "Step 11: Loss 6.827768325805664\n",
      "Step 12: Loss 6.938750267028809\n",
      "Step 13: Loss 6.851911544799805\n",
      "Step 14: Loss 7.289831161499023\n",
      "Step 15: Loss 6.646097183227539\n",
      "Step 16: Loss 7.276329040527344\n",
      "Step 17: Loss 7.230772495269775\n",
      "Step 18: Loss 7.1962785720825195\n",
      "Step 19: Loss 6.674407005310059\n",
      "Step 20: Loss 6.943556785583496\n",
      "Step 21: Loss 6.847167015075684\n",
      "Step 22: Loss 6.741520404815674\n",
      "Step 23: Loss 6.494239330291748\n",
      "Step 24: Loss 6.903376579284668\n",
      "Step 25: Loss 6.663928985595703\n",
      "Step 26: Loss 6.943594932556152\n",
      "Step 27: Loss 7.129715919494629\n",
      "Step 28: Loss 7.358424663543701\n",
      "Step 29: Loss 6.478534698486328\n",
      "Step 30: Loss 6.625563621520996\n",
      "Step 31: Loss 6.6931962966918945\n",
      "Step 32: Loss 7.058719635009766\n",
      "Step 33: Loss 6.738534927368164\n",
      "Step 34: Loss 6.876980304718018\n",
      "Step 35: Loss 6.862082004547119\n",
      "Step 36: Loss 6.844743251800537\n",
      "Step 37: Loss 7.152878761291504\n",
      "Step 38: Loss 6.955892562866211\n",
      "Step 39: Loss 6.9094414710998535\n",
      "Step 40: Loss 7.03419303894043\n",
      "Step 41: Loss 6.748800277709961\n",
      "Step 42: Loss 7.009777069091797\n",
      "Step 43: Loss 7.5786566734313965\n",
      "Step 44: Loss 6.940283298492432\n",
      "Step 45: Loss 6.679618835449219\n",
      "Step 46: Loss 7.284014701843262\n",
      "Step 47: Loss 6.77454948425293\n",
      "Step 48: Loss 6.753704071044922\n",
      "Step 49: Loss 6.6473708152771\n",
      "Step 50: Loss 6.822854518890381\n"
     ]
    }
   ],
   "source": [
    "optimiser = torch.optim.AdamW(model.parameters(), lr=3e-10)\n",
    "steps = 50\n",
    "for i in range(steps):\n",
    "    x, y = training_loader.get_next_batch()\n",
    "    optimiser.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    print(f\"Step {i+1}: Loss {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, that will be able to train on the shakespeare dataset, although we'd need to train it a bunch more for it to be able to generate anything useful since it's a huge model and transformers need exponentially more data with the size of the model. Still I'll put some sample outputs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# x = torch.tensor(enc.encode(\"Montague:\")).repeat(5,1).to(device)\n",
    "# while x.size(1) < max_length:\n",
    "#     with torch.no_grad():\n",
    "#         logits, loss = model(x)\n",
    "#         # Take the logits at the last position for the entire batch\n",
    "#         # with the entire vocab_size\n",
    "#         logits= logits[:, -1, :]\n",
    "#         # Softmax over the vocab_size to prepare for sampling\n",
    "#         probs = F.softmax(logits, dim=-1)\n",
    "#         # Since we want to generate exactly as hugging face does\n",
    "#         # below code is the same as HF.\n",
    "#         # Isolate the top 50 most likely words to avoid even super improbable\n",
    "#         # words from being a candidate. This avoids model random slip ups\n",
    "#         top_k_probs, top_k_indices = torch.topk(probs, 50, dim=-1) # of size (5,50)\n",
    "#         # Sample one random word from the top 50\n",
    "#         chosen_index = torch.multinomial(top_k_probs, 1) # of size (B, 1)\n",
    "#         # Find corresponding indices of chosen indexes\n",
    "#         xcol = torch.gather(top_k_indices, -1, chosen_index)\n",
    "#         # Put all the collected indexes together\n",
    "#         x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# for i in range(num_return_sequences):\n",
    "#     tokens = x[i, :max_length].tolist()\n",
    "#     decoded = enc.decode(tokens)\n",
    "#     print(f\"Generation {i+1}:\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another change aside from the pre-norm is that the wte and the final lm_head both generate the final embeddings of output and hence they are best to be shared across both components of the transformer. This is a common practice in transformers today since they give better performance, but also save us an insane number of parameters. In our case for example, we are saving 768 * 50257 = 38,597,376 parameters, which is a lot since our model is only 124M parameters (saving on 30% of parameters).\n",
    "\n",
    "In addition to the shared weights, we will also initialise the model weights the same was as in GPT2. In this case, we will just use the stuff from OpenAI but since their value is about what is equal to the Xavier initialisation (1/sqrt(n_in)) but the average value for it across all the models they trained with the GPT2 architecture. This initialisation is performed on the linear components of the model and the Embeddings. The layer norm components already initialise to 1 and 0 by default in PyTorch so nothing to worry about there.\n",
    "\n",
    "The final important element of the weight initialisation is to the residual streams. The paper follows an initialisation of each value by 1/sqrt(n_layers) because the residual streams are added at each layer. Consider the example below:\n",
    "```python\n",
    "residual_stream_target = torch.zeros(n_embd)\n",
    "n_layers = 100\n",
    "for i in range(n_layers):\n",
    "    residual_stream_target += torch.randn(n_embd)\n",
    "print(residual_stream_target.std()) # will print a value around 10\n",
    "```\n",
    "Here the residual stream target will actually start to expand in the standard deviation until the final standard deviation of the residual stream target will be equal to around sqrt(n_layers). In our blocks, this will mean that by the end of all of our layers, our data stream will have much higher standard deviation than 1. In order to avoid this, we will ensure that each time a residual stream is added to the target data stream, we will divide by sqrt(n_layers).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to implement the Residual Initialisation, we will need to modify the forward pass to add the residual stream initialisation. First we will need to create a custom flag in the module that will flag when a residual stream is present. Since we have two residual steams in each block, we will need to scale the initialised weights to have a standard deviation of 1/sqrt((2 * n_layers))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        # Expanding the dimensions of the data to let some \n",
    "        # computation occur\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        # gelu activation uses a tanh to approximate the real function\n",
    "        # in the GPT2 due to an error in PyTorch but we need the exact\n",
    "        # same stuff to import properly so we gotta suffer same way\n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "        # Projecting the data back into normal dimensions\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        \n",
    "        # Adding a custom flag to the model to flag our scaling\n",
    "        self.c_proj.CUSTOM_SCALING_INIT = 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No changes made to the block at this stage but it's just here for completeness and less confusion lol. Look at the residual streams in effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Residual skip connection for the attention block with pre-norm\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        # Residual skip connection for the MLP block with pre-norm\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's implement the other initialisations at the GPT level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            # dictionary of token embedding weights (weight of token embeddings)\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            # dictionary of positional embedding weights (weight of positional embeddings)\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            # Attention blocks\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            # Final Layer norm (since pre-norm doesn't touch final block output)\n",
    "            ln_f = nn.LayerNorm(config.n_embd)\n",
    "        ))\n",
    "        # Linear projection out of the Attention block\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Weight sharing between wte and lm_head \n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        \n",
    "        # Apply weight initialisation\n",
    "        self.apply(self._init_weights) \n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        # Initialise the weights of the model using the same method as in GPT2\n",
    "        std = 0.02\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if hasattr(module, 'CUSTOM_SCALING_INIT'):\n",
    "                std = (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "    \n",
    "    def forward(self, text, targets=None):\n",
    "        # Raw data input of b context length sized sequences of raw text\n",
    "        B, T = text.size()\n",
    "        assert T <= self.config.block_size, \"Sequence length too high\"\n",
    "        # Create numbers from 0 to T\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=text.device)\n",
    "        # Fetch positional and token embeddings for all the values in text\n",
    "        pos_emb = self.transformer.wpe(pos) # of size (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(text) # of size (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # of size (B, T, vocab_size)\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tokens in dataset: 338025\n",
      "batches to 1 epoch: 2640\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "training_loader = DataLoader(B=4, T=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected device: mps\n"
     ]
    }
   ],
   "source": [
    "model = GPT(GPTConfig())\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "print(f\"Detected device: {device}\")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loss 10.979299545288086\n",
      "Step 2: Loss 9.898200988769531\n",
      "Step 3: Loss 9.00602912902832\n",
      "Step 4: Loss 9.23298168182373\n",
      "Step 5: Loss 8.776897430419922\n",
      "Step 6: Loss 8.455888748168945\n",
      "Step 7: Loss 9.124743461608887\n",
      "Step 8: Loss 8.800727844238281\n",
      "Step 9: Loss 8.292373657226562\n",
      "Step 10: Loss 8.127102851867676\n",
      "Step 11: Loss 8.415895462036133\n",
      "Step 12: Loss 7.431976318359375\n",
      "Step 13: Loss 7.954823017120361\n",
      "Step 14: Loss 7.540158271789551\n",
      "Step 15: Loss 7.561570167541504\n",
      "Step 16: Loss 7.433925628662109\n",
      "Step 17: Loss 7.404224395751953\n",
      "Step 18: Loss 8.311769485473633\n",
      "Step 19: Loss 7.231884002685547\n",
      "Step 20: Loss 7.792387962341309\n",
      "Step 21: Loss 7.490170478820801\n",
      "Step 22: Loss 7.811254024505615\n",
      "Step 23: Loss 6.378418922424316\n",
      "Step 24: Loss 6.798283576965332\n",
      "Step 25: Loss 6.816643238067627\n",
      "Step 26: Loss 6.6144208908081055\n",
      "Step 27: Loss 6.713146209716797\n",
      "Step 28: Loss 7.622711658477783\n",
      "Step 29: Loss 7.163756370544434\n",
      "Step 30: Loss 6.938514709472656\n",
      "Step 31: Loss 6.92976188659668\n",
      "Step 32: Loss 7.205760955810547\n",
      "Step 33: Loss 7.074167251586914\n",
      "Step 34: Loss 6.857917785644531\n",
      "Step 35: Loss 7.9071879386901855\n",
      "Step 36: Loss 7.766783714294434\n",
      "Step 37: Loss 7.554133892059326\n",
      "Step 38: Loss 7.595660209655762\n",
      "Step 39: Loss 7.749884128570557\n",
      "Step 40: Loss 7.338174819946289\n",
      "Step 41: Loss 7.379976749420166\n",
      "Step 42: Loss 6.618063926696777\n",
      "Step 43: Loss 6.930643081665039\n",
      "Step 44: Loss 6.917328357696533\n",
      "Step 45: Loss 6.919079780578613\n",
      "Step 46: Loss 7.019636154174805\n",
      "Step 47: Loss 5.966238021850586\n",
      "Step 48: Loss 6.15715217590332\n",
      "Step 49: Loss 6.880684852600098\n",
      "Step 50: Loss 6.699258804321289\n"
     ]
    }
   ],
   "source": [
    "optimiser = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "steps = 50\n",
    "for i in range(steps):\n",
    "    x, y = training_loader.get_next_batch()\n",
    "    optimiser.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    print(f\"Step {i+1}: Loss {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now at this stage, we actually have a fully functional GPT2 model. The thing we're gonna do that's gonna be very cash money is going to be the optimisations we will apply from here on out. This checkpoint is in the gpt-2-train-raw.py file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
